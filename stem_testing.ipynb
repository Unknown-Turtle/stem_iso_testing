{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Unknown-Turtle/stem_iso_testing/blob/main/stem_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6mcSc0mmp3i"
      },
      "source": [
        "# Using spleeter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8Brdfh6mzEz"
      },
      "outputs": [],
      "source": [
        "!apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_6Ram1lmc1F"
      },
      "outputs": [],
      "source": [
        "pip install spleeter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0LktyMypXqE"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afbcUSken16L"
      },
      "source": [
        "# Separate from command line"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: i've only done this on mac with a conda venv"
      ],
      "metadata": {
        "id": "l_Db0dF8-Djj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1kQaoJSoAD0"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibG6uF55p4lH"
      },
      "outputs": [],
      "source": [
        "Audio('audio.mp3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOAqBcPhn6IU"
      },
      "outputs": [],
      "source": [
        "!spleeter separate -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGL-k5xxoKbu"
      },
      "outputs": [],
      "source": [
        "!spleeter separate -o output/ audio_example.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDuPWcAMoZP_"
      },
      "outputs": [],
      "source": [
        "!ls output/audio_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7CHpyiloxrk"
      },
      "outputs": [],
      "source": [
        "Audio('output/audio/vocals.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibXd-WCTpT0w"
      },
      "outputs": [],
      "source": [
        "Audio('output/audio/accompaniment.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using demucs"
      ],
      "metadata": {
        "id": "MLsQ8xpu-iyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install demucs -q\n",
        "!apt-get install ffmpeg -y"
      ],
      "metadata": {
        "id": "_ysYw6uX_Uqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic usage: separate into 4 stems (vocals, drums, bass, other)\n",
        "!demucs audio.mp3"
      ],
      "metadata": {
        "id": "7prUTmqiBWlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 4. Using different models\n",
        "# htdemucs (Hybrid Transformer) model, higher quality\n",
        "!demucs -n htdemucs -o output audio.mp3"
      ],
      "metadata": {
        "id": "BdYZXQhWB13D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6-stem model for more detailed separation (vocals, drums, bass, piano, guitar, other)\n",
        "!demucs -n htdemucs_6s -o output audio.mp3"
      ],
      "metadata": {
        "id": "3VgYOvERB4kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 5. Only separate vocals vs everything else\n",
        "!demucs --two-stems=vocals -o output audio.mp3"
      ],
      "metadata": {
        "id": "7gkMnIGRB6QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Music21\n"
      ],
      "metadata": {
        "id": "GEBDomdhCYj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a more fleshed out example, include more analyis, needs a bit of fixing up though. probably should remove unusded imports and edit the main guard"
      ],
      "metadata": {
        "id": "-so38fe92-Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, math, pathlib\n",
        "import numpy as np\n",
        "import librosa, librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import normalize\n",
        "from music21 import key as m21key\n",
        "\n",
        "# Configs\n",
        "SR = 22050\n",
        "HOP = 512\n",
        "N_FFT = 2048\n",
        "# Chord decoding params\n",
        "FRAME_SEC = HOP / SR\n",
        "SMOOTH_FRAMES = 9  # median smoothing for chord labels\n",
        "\n",
        "# a\n",
        "def load_audio(path):\n",
        "    y, sr = librosa.load(path, sr=SR, mono=True)\n",
        "    y = librosa.util.normalize(y)\n",
        "    return y, sr\n",
        "\n",
        "# spectrogram\n",
        "def compute_mel_spec(y, sr):\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP, n_mels=128, power=2.0)\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "    return S_db\n",
        "\n",
        "# Onset Detection\n",
        "def detect_onsets(y, sr):\n",
        "    onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=HOP, aggregate=np.median)\n",
        "    onsets_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, hop_length=HOP, units='frames', backtrack=True)\n",
        "    onset_times = librosa.frames_to_time(onsets_frames, sr=sr, hop_length=HOP).tolist()\n",
        "    return onset_times, onset_env\n",
        "\n",
        "# Key Estimation (simple chroma template)\n",
        "MAJOR_TEMPLATE = np.array([6,2,3,2,4,2,3,6,2,4,2,3], dtype=float)\n",
        "MINOR_TEMPLATE = np.array([6,2,3,6,2,4,2,3,6,2,4,2], dtype=float)\n",
        "\n",
        "def estimate_key(y, sr):\n",
        "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=HOP, bins_per_octave=36)\n",
        "    chroma_mean = chroma.mean(axis=1)\n",
        "    chroma_mean = chroma_mean / (chroma_mean.sum() + 1e-9)\n",
        "\n",
        "    best_score, best_key = -1, None\n",
        "    for tonic in range(12):\n",
        "        major = np.roll(MAJOR_TEMPLATE, tonic)\n",
        "        minor = np.roll(MINOR_TEMPLATE, tonic)\n",
        "        major = major / major.sum()\n",
        "        minor = minor / minor.sum()\n",
        "        sM = np.dot(chroma_mean, major)\n",
        "        sN = np.dot(chroma_mean, minor)\n",
        "        if sM > best_score:\n",
        "            best_score, best_key = sM, (tonic, 'major')\n",
        "        if sN > best_score:\n",
        "            best_score, best_key = sN, (tonic, 'minor')\n",
        "\n",
        "    pitch_names = ['C','C#','D','Eb','E','F','F#','G','Ab','A','Bb','B']\n",
        "    tonic_name = pitch_names[best_key[0]]\n",
        "    music21_guess = m21key.Key(tonic_name, best_key[1]).tonic.name + ' ' + best_key[1]\n",
        "    return {'tonic_index': best_key[0], 'mode': best_key[1], 'name': music21_guess}\n",
        "\n",
        "# Chord Estimation (triad template matching per frame)\n",
        "# 24 triad templates (12 major + 12 minor).\n",
        "# might extend to 7ths later...\n",
        "def build_triad_templates():\n",
        "    T = []\n",
        "    names = []\n",
        "    # intervals (major: 0,4,7), (minor: 0,3,7)\n",
        "    for i, mode in [(0,'major'), (1,'minor')]:\n",
        "        for tonic in range(12):\n",
        "            tpl = np.zeros(12)\n",
        "            if mode == 'major':\n",
        "                intervals = [0,4,7]\n",
        "                name = ['', 'm'][i]\n",
        "            else:\n",
        "                intervals = [0,3,7]\n",
        "                name = 'm'\n",
        "            for iv in intervals:\n",
        "                tpl[(tonic+iv)%12] = 1.0\n",
        "            T.append(tpl)\n",
        "            root_names = ['C','C#','D','Eb','E','F','F#','G','Ab','A','Bb','B']\n",
        "            names.append(root_names[tonic] + ('' if mode=='major' else 'm'))\n",
        "    T = np.array(T, dtype=float)\n",
        "    T = normalize(T, norm='l1', axis=1)\n",
        "    return T, names\n",
        "\n",
        "TEMPLATES, CHORD_NAMES = build_triad_templates()\n",
        "\n",
        "def frame_chords(y, sr):\n",
        "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=HOP, bins_per_octave=36)  # (12, frames)\n",
        "    chroma = normalize(chroma + 1e-6, norm='l1', axis=0)\n",
        "    # cosine similarity to templates\n",
        "    sims = TEMPLATES @ chroma  # (24, frames)\n",
        "    idx = np.argmax(sims, axis=0)\n",
        "    return idx, CHORD_NAMES, chroma\n",
        "\n",
        "def smooth_labels(idx, k=SMOOTH_FRAMES):\n",
        "    from scipy.ndimage import median_filter\n",
        "    return median_filter(idx, size=k, mode='nearest')\n",
        "\n",
        "def chord_segments(idx_smooth, names):\n",
        "    # Convert per-frame labels into (start_time, end_time, chord)\n",
        "    segs = []\n",
        "    f2t = lambda f: f * FRAME_SEC\n",
        "    start = 0\n",
        "    for i in range(1, len(idx_smooth)):\n",
        "        if idx_smooth[i] != idx_smooth[i-1]:\n",
        "            segs.append((f2t(start), f2t(i), names[idx_smooth[i-1]]))\n",
        "            start = i\n",
        "    segs.append((f2t(start), f2t(len(idx_smooth)), names[idx_smooth[-1]]))\n",
        "    return segs\n",
        "\n",
        "# Pattern mining\n",
        "def ngram_stats(chord_seq, n=2):\n",
        "    from collections import Counter\n",
        "    grams = [tuple(chord_seq[i:i+n]) for i in range(len(chord_seq)-n+1)]\n",
        "    c = Counter(grams)\n",
        "    total = sum(c.values())\n",
        "    return [{'pattern':' â†’ '.join(g), 'count':cnt, 'pct':round(100*cnt/total,2)} for g,cnt in c.most_common(12)]\n",
        "\n",
        "# plot graphs\n",
        "def plot_spectrogram_with_onsets(S_db, onset_times, sr):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    librosa.display.specshow(S_db, sr=sr, hop_length=HOP, x_axis='time', y_axis='mel')\n",
        "    for t in onset_times:\n",
        "        plt.axvline(t, linewidth=0.8, alpha=0.7)\n",
        "    plt.title('Mel Spectrogram with Onset Lines')\n",
        "    plt.colorbar(format=\"%+2.0f dB\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_chromagram(chroma):\n",
        "    plt.figure(figsize=(12,3))\n",
        "    librosa.display.specshow(chroma, x_axis='time', y_axis='chroma', hop_length=HOP)\n",
        "    plt.title('Chromagram')\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "\n",
        "# ---------- MAIN ----------\n",
        "def analyze(piano_path, out_json='analysis.json', show_plots=True):\n",
        "    print(\"[1/8] Selecting audio source...\")\n",
        "    src, used_nv = pick_source(piano_path)\n",
        "    print(f\"Using: {src}\")\n",
        "\n",
        "    print(\"[2/8] Loading audio...\")\n",
        "    y, sr = load_audio(src)\n",
        "    print(f\"Loaded {len(y)/sr:.1f} seconds @ {sr} Hz\")\n",
        "\n",
        "    print(\"[3/8] Computing mel spectrogram...\")\n",
        "    S_db = compute_mel_spec(y, sr)\n",
        "\n",
        "    print(\"[4/8] Detecting onsets...\")\n",
        "    onset_times, onset_env = detect_onsets(y, sr)\n",
        "    print(f\"Found {len(onset_times)} onsets\")\n",
        "\n",
        "    print(\"[5/8] Estimating key signature...\")\n",
        "    key_info = estimate_key(y, sr)\n",
        "    print(f\"Estimated key: {key_info['name']}\")\n",
        "\n",
        "    print(\"[6/8] Detecting chords and smoothing...\")\n",
        "    idx, names, chroma = frame_chords(y, sr)\n",
        "    idx_s = smooth_labels(idx)\n",
        "    segments = chord_segments(idx_s, names)\n",
        "    print(f\"{len(segments)} chord segments found\")\n",
        "\n",
        "    print(\"[7/8] Extracting common progressions...\")\n",
        "    chord_seq = [n for _, _, n in segments]\n",
        "    bigrams = ngram_stats(chord_seq, n=2)\n",
        "    trigrams = ngram_stats(chord_seq, n=3) if len(chord_seq) >= 3 else []\n",
        "\n",
        "    print(\"[8/8] Generating plots and saving output...\")\n",
        "    plot_spectrogram_with_onsets(S_db, onset_times, sr, HOP)\n",
        "    plot_chromagram(chroma, HOP)\n",
        "    plot_chord_timeline(idx_s, segments, FRAME_SEC)\n",
        "\n",
        "    result = {\n",
        "        \"source_used\": \"no_vocals.wav\" if used_nv else pathlib.Path(src).name,\n",
        "        \"sample_rate\": sr,\n",
        "        \"hop_length\": HOP,\n",
        "        \"onset_times_sec\": onset_times,\n",
        "        \"key\": key_info,\n",
        "        \"chord_segments\": [{\"start\": round(a, 3), \"end\": round(b, 3), \"chord\": c} for a, b, c in segments],\n",
        "        \"progressions_top_bigrams\": bigrams,\n",
        "        \"progressions_top_trigrams\": trigrams,\n",
        "    }\n",
        "\n",
        "    pathlib.Path(OUT_DIR / out_json).write_text(json.dumps(result, indent=2))\n",
        "    print(f\"Analysis complete. Results saved in: {OUT_DIR.resolve()}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python vookley_piano_analyzer.py piano.wav\")\n",
        "        sys.exit(1)\n",
        "    analyze(sys.argv[1])\n"
      ],
      "metadata": {
        "id": "sjc4_MqgCqxP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "spleeter.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
